---
title: "Project_Final"
author: "Deviprasad Saka"
date: "2024-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset/data

#About Dataset

#Context
This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The "target" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.

#Content
Attribute Information:

age
sex
chest pain type (4 values)
resting blood pressure
serum cholestoral in mg/dl
fasting blood sugar > 120 mg/dl
resting electrocardiographic results (values 0,1,2)
maximum heart rate achieved
exercise induced angina
oldpeak = ST depression induced by exercise relative to rest
the slope of the peak exercise ST segment
number of major vessels (0-3) colored by flourosopy
thal: 0 = normal; 1 = fixed defect; 2 = reversable defect
The names and social security numbers of the patients were recently removed from the database, replaced with dummy values.

```{r}
library(tidyverse)
library(readr)
library(ROCR)
library(PerformanceAnalytics)
library(e1071)
library(caret)
library(gbm)
library(corrplot)
library(ggcorrplot)
library(MASS)
library(rpart)
library(caTools)
library(naivebayes)
library(class)
library(ISLR)
library(glmnet)
library(Hmisc)
library(funModeling)
library(pROC)
library(randomForest)
library(klaR)
library(scales)
library(cluster)
library(factoextra)
library(DataExplorer)
library(ClustOfVar)
library(GGally)
library(dplyr)
library(visdat)
library(naniar)
library(mice)
library(VIM)
```


(EDA) Exploratory Data Analysis: 

```{r}

file_path <- "D:/Multivariate Analysis/Final_Project/heart.csv"
data <- read.csv(file_path)
head(data)
```

#Missing Values
```{r}
#plot_missing(data)
vis_miss(data)

```




#Transformation
```{r}

data2 <- data %>% 
  mutate(sex = if_else(sex == 1, "MALE", "FEMALE"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "YES" ,"NO"),
         cp = if_else(cp == 1, "ATYPICAL ANGINA",
                      if_else(cp == 2, "NON-ANGINAL PAIN", "ASYMPTOMATIC")),
         restecg = if_else(restecg == 0, "NORMAL",
                           if_else(restecg == 1, "ABNORMALITY", "PROBABLE OR DEFINITE")),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal),
         target = if_else(target == 1, "YES", "NO")
         ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())

```


This code modifies the dataset `data` by recoding certain variables and converting them into factors or changing their labels. Here's what each line does:
The changes made are:
- Recoding binary variables into descriptive categories (e.g., `sex`, `fbs`, `exang`).
- Recoding categorical variables into more readable labels (e.g., `cp`, `restecg`).
- Converting certain variables into factors (e.g., `slope`, `ca`, `thal`).
- Converting the `target` variable into "YES" or "NO".
- Converting all character variables into factors.
- Rearranging columns so that `target`, `sex`, `fbs`, `exang`, `cp`, `restecg`, `slope`, `ca`, `thal`, and other variables are selected in that order.

#Summary Statistics
```{r}
summary(data2)

```


#Boxplot
```{r}
# Define colors for boxplots
colors <- c("red", "blue", "green", "orange", "purple")

# Plot boxplots with different colors
boxplot(data2[, 10:14], col = colors)

```


#Data Visualization

# Bar plot for target (Heart disease)
```{r}
ggplot(data2, aes(x=data2$target, fill=data2$target)) + 
  geom_bar() +
  xlab("Heart Disease") +
  ylab("Count") +
  ggtitle("Analysis of Presence and Absence of Heart Disease") +
  scale_fill_discrete(name = "Heart Disease", labels = c("Absence", "Presence"))

```






```{r}
prop.table(table(data2$target))

```

# Bar chart of Number of Major Vessels Distribution based on Exercise Induced Angina
```{r}
library(ggplot2)
ggplot(data2, aes(x = exang, fill = ca)) +
  geom_bar(position = "dodge", color = "black") +
  labs(x = "Exercise Induced Angina", y = "Count", fill = "Number of Major Vessels Distribution") +
  theme_minimal() +
  theme(legend.position = "right",
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        legend.title = element_text(size = 10))

```

#Pair Plot of Heart Data with target
```{r}
# Subset the data to include only the required variables
heart_subset <- data2[, c('age', 'sex', 'cp', 'trestbps', 'chol', 'target')]

# Define colors for the target variable
colors <- c("HEALTHY" = "#1F77B4", "UNHEALTHY" = "#FF7F0E")

# Convert target variable to factor
heart_subset$target <- factor(heart_subset$target)

# Create pair plot with different color for each target category
pairs(heart_subset[, c('age', 'sex', 'cp', 'trestbps', 'chol')],
      col = colors[as.numeric(heart_subset$target)],
      main = "Pair Plot of Heart Data with target",
      pch = 19)


```

This pair plot visualizes the pairwise relationships between five variables (age, sex, cp, trestbps, and chol) from a heart disease dataset, along with the target variable indicating the presence or absence of heart disease.

Age: The scatter plots involving age show a clear separation between the orange (healthy) and blue (unhealthy) points, with unhealthy individuals tending to be older. The density plot for age also exhibits a bimodal distribution, suggesting two distinct age groups in the data.

Sex: The scatter plots with sex, which is a binary variable (0 for female, 1 for male), show a higher concentration of unhealthy (blue) points for males compared to females, indicating a potential relationship between gender and heart disease risk.

Cp (chest pain type): The scatter plots with cp suggest a possible correlation between certain types of chest pain and the target variable, as indicated by the clustering of colors.

Trestbps (resting blood pressure): The scatter plots involving trestbps show a tendency for higher blood pressure values to be associated with the unhealthy (blue) points, implying a potential link between elevated blood pressure and heart disease risk.

Chol (cholesterol level): The scatter plots and density plot for chol exhibit a clear separation between healthy and unhealthy individuals, with higher cholesterol levels being more prevalent among the unhealthy (blue) points.

# Density plot of resting blood pressure based on slope
```{r}

library(ggplot2)
# Density plot of resting blood pressure based on slope
ggplot(data2, aes(x = trestbps, fill = slope)) +
  geom_density(alpha = 0.6, color = "black") +
  labs(x = "Resting Blood Pressure (mm Hg)", y = "Density") +
  facet_wrap(~ slope, scales = "free_y") +
  theme_minimal() +
  theme(legend.position = "bottom",
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        legend.title = element_text(size = 10),
        strip.text = element_text(size = 10, face = "bold"))


```

Here are some observations about the individual plots:

The red plot (Category 0) shows a peak slightly lower than 120 mm Hg.

The green plot (Category 1) has its peak around 130 mm Hg and appears to have a broader spread than the red plot.

The blue plot (Category 2) has a peak just below 140 mm Hg and the narrowest spread of the three, indicating that the resting blood pressure for this group is more concentrated around the mean.

# Counting the frequency of the values of the age
```{r}
library(ggplot2)
library(gridExtra)

# Subset the data and convert 'slope' to character
df_eda5 <- data2[, c('slope', 'trestbps')]
df_eda5$slope <- as.character(df_eda5$slope)

# Unique slope values and color palette
slope_list <- sort(unique(df_eda5$slope))
color_palette <- c("#1f77b4", "#ff7f0e", "#2ca02c")

# Main KDE Plot
main_plot <- ggplot(df_eda5, aes(x = trestbps, fill = slope)) +
  geom_density(alpha = 0.4, bw = 0.4) +
  scale_fill_manual(values = color_palette) +
  theme_minimal() +
  labs(x = "\nResting Blood Pressure (in mm Hg)", y = "Density\n") +
  theme(legend.position = "none")

# Sub Q-Q Plot
sub_plots <- lapply(seq_along(slope_list), function(i) {
  slope <- slope_list[i]
  df_slope <- df_eda5[df_eda5$slope == slope, ]
  qq_plot <- ggplot(df_slope, aes(sample = trestbps)) +
    stat_qq(color = color_palette[i], size = 0.8, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
    theme_minimal() +
    labs(title = paste("Q-Q Plot - Slope", slope), x = "", y = "") +
    theme(legend.position = "none")
  return(qq_plot)
})

# Combine plots
grid.arrange(main_plot, grobs = sub_plots, nrow = 2, align = "v")

# Note: Adjust plot settings (e.g., font sizes, titles, labels) as needed for your specific visualization.
```

Q-Q Plot - Slope 0: The blue dots represent the quantiles of the dataset for the category "Slope 0." The points do not align with a straight line, suggesting that the data for this category may not be normally distributed.

Q-Q Plot - Slope 1: The orange dots for "Slope 1" show a clearer trend of the points lining up along an approximately straight diagonal line. This indicates that the data for "Slope 1" is closer to a normal distribution compared to "Slope 0."

Q-Q Plot - Slope 2: The green dots for "Slope 2" also line up closely with a straight diagonal line, similar to "Slope 1," suggesting that the data for "Slope 2" is also approximately normally distributed.

```{r}
library(ggplot2)
library(dplyr)

# Group the age columns of the analysis by age_group
data2$age_group <- cut(data2$age, breaks = seq(25, 75, 5))

# Calculate heart disease proportion for each age group
age_group <- data2 %>%
  group_by(age_group) %>%
  summarise(heart_dis_prop = round(sum(target == "YES") / n(), 3) * 100)  %>%
  na.omit()

# Plot the heart disease proportion by age group
ggplot(age_group, aes(x = age_group, y = heart_dis_prop, fill = age_group)) +
  geom_col(position = "dodge") + 
  labs(x = "Age Group", y = "Heart Disease Proportion", title = "Age Group Heart Disease Proportion")


```

This graph is a vertical bar chart titled "Age Group Heart Disease Proportion." It shows the proportion of heart disease across different age groups, where each age group is represented by a different color, as indicated in the legend on the right side of the graph.

The y-axis indicates "Heart Disease Proportion" and is measured from 0 to 100, suggesting a percentage scale, although the actual units are not specified. The x-axis lists the age groups in intervals of 5 years.

The height of each bar represents the proportion of heart disease in that age group. While the graph lacks specific values on each bar, the visual representation shows an interesting trend: the proportion of heart disease generally increases with age, as the bars get taller in the higher age groups, with the (70,75) age group showing the highest proportion of all.

This graph could be used to analyze the prevalence of heart disease among different age demographics, potentially for use in public health planning, resource allocation, or to highlight the importance of preventive measures for certain age groups.

```{r}
library(ggplot2)

# Modify the plot
ggplot(data2, aes(x = factor(cp), fill = factor(target))) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "Chest Pain Type", y = "Proportion", fill = "Target") +
  ggtitle("Proportion of Target by Chest Pain Type") +
  scale_fill_manual(values = c("NO" = "#4CAF50", "YES" = "#F44336"),
                    labels = c("No Heart Disease", "Heart Disease")) +
  theme_minimal()

```

From the plot, we can observe the following:

In the ASYMPTOMATIC group, the proportion of individuals with heart disease (red) is less than the proportion without heart disease (green).

The ATYPICAL ANGINA group has a higher proportion of individuals with heart disease compared to those without.

The NON-ANGINAL PAIN group seems to have a roughly equal proportion of individuals with and without heart disease.

#Distribution of Male and Female population across Age parameter
```{r}
library(ggplot2)

# Create separate plots for male and female populations
data2 %>%
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  xlab("Age") +
  ylab("Number") +
  facet_wrap(~ sex, nrow = 1) +  # Create separate plots for each sex
  theme_minimal()  # Use minimal theme for cleaner appearance
```

For the "FEMALE" category, there are multiple bars of varying heights, indicating the frequency of different age groups. The ages appear to range from around 30 to over 70. The distribution seems fairly uniform with a slight concentration around the 50-60 age range.

For the "MALE" category, the ages also range from around 30 to over 70. However, there is a more pronounced peak in the distribution around the 50-60 age range, indicating a higher frequency of males in that age group compared to females.

The plot is designed to allow for easy comparison between the age distributions of females and males. It suggests that while both distributions cover similar age ranges, there are differences in the frequency of certain age groups between genders.

#Density plot of target with respect to variables 
```{r}
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Assuming your dataset is named data2
# Convert target variable to a factor with levels "NO" and "YES"
data2$target <- factor(data2$target, levels = c("NO", "YES"))

# Define a function to create density plots
plot_density <- function(data, var) {
  ggplot(data, aes(x = !!as.name(var), fill = target)) +
    geom_density(alpha = 0.5) +
    labs(title = paste("Density plot of", var, "by target")) +
    theme_minimal()
}

# Create density plots for all numeric variables
plots <- lapply(names(data2)[sapply(data2, is.numeric)], function(var) {
  plot_density(data2, var)
})

# Arrange plots in a grid layout
grid.arrange(grobs = plots, ncol = 3)
```

Density plot of age by target: This plot shows the density distribution of 'age' for two groups. It appears that the group with 'target' value "YES" tends to have a slightly younger age distribution than the group with 'target' value "NO".

Density plot of trestbps by target: This plot represents the density distribution of 'trestbps', which could stand for resting blood pressure. Here, the distributions for both "YES" and "NO" categories are somewhat similar, with the "YES" group having a slightly higher density around the mode.

Density plot of chol by target: This plot shows the density distribution of 'chol', which may stand for cholesterol levels. Both groups have similar shapes, but the "NO" group seems to have a slightly higher density in the mid-range of cholesterol levels.

Density plot of thalach by target: Here, the variable 'thalach' could represent maximum heart rate achieved. The "YES" group shows a higher density for higher values of 'thalach', indicating that this group may have higher maximum heart rates.

Density plot of oldpeak by target: The variable 'oldpeak' might refer to ST depression induced by exercise relative to rest. In this plot, the "YES" group has a higher density at lower 'oldpeak' values, which could suggest less ST depression during exercise.

# Representation of Cholesterol level 
```{r}
library(ggplot2)

# Modify the plot
data2 %>%
  ggplot(aes(x = age, y = chol, color = sex, size = chol)) +
  geom_point(alpha = 0.7) +
  labs(x = "Age", y = "Cholesterol", color = "Sex") +  # Change axis and legend labels
  scale_color_manual(values = c("blue", "red"), labels = c("Female", "Male")) +  # Customize color scale
  scale_size_continuous(range = c(2, 6)) +  # Adjust point size range
  theme_minimal()  # Use minimal theme for cleaner appearance
```

The image shows a scatter plot with "Age" on the x-axis and "Cholesterol" on the y-axis. The data points are color-coded to represent two different sexes: blue for female and red for male. Additionally, the size of each data point corresponds to different cholesterol levels, with larger dots indicating higher cholesterol levels.

# Comparison of Blood pressure across pain type 
```{r}

library(ggplot2)

# Modify the plot
data2 %>%
  ggplot(aes(x = sex, y = trestbps, fill = factor(cp))) +
  geom_boxplot() +
  scale_fill_brewer(palette = "Set1") +  # Use a color palette for pain types
  labs(x = "Sex", y = "Blood Pressure (BP)", fill = "Pain Type") +  # Change axis and legend labels
  theme_minimal() +  # Use minimal theme for a cleaner appearance
  facet_grid(~cp)  # Facet by pain type

```


```{r}
library(ggplot2)

ggplot(data2) +
  geom_histogram(aes(x = oldpeak, y = ..density.., fill = target), 
                 position = "dodge", color = "black", alpha = 0.7, bins = 20) +
  geom_density(aes(x = oldpeak, fill = "Density"), alpha = 0.3, color = "red") +
  scale_fill_manual(values = c("NO" = "blue", "YES" = "green", "Density" = "orange")) +
  labs(x = "Oldpeak", y = "Density", fill = "") +  # Adjust axis labels and legend title
  theme_minimal()  # Use minimal theme for cleaner appearance
```

The histogram bars show the distribution of the Oldpeak values for two different groups (YES and NO). The height of each bar corresponds to the density (or frequency) of observations within that range of Oldpeak values. The density plot, which is represented by a red line, shows the probability density function of the Oldpeak variable, giving a sense of the continuous probability distribution across the values.

```{r}
library(corrplot)
# Calculate the correlation matrix
cor_heart <- cor(data2[, 10:14])

# Display the correlation matrix
cor_heart

# Plot the correlation matrix in a heatmap
corrplot(cor_heart, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45,
         addCoef.col = "black", number.cex = 0.7)
```

The diagonal from the top left to the bottom right shows perfect correlations (1.0) because these are the variables correlated with themselves.
'age' has a moderate positive correlation with 'trestbps' (0.27) and a weak positive correlation with 'chol' (0.22).
'thalach' has a moderate negative correlation with 'age' (-0.39) and with 'oldpeak' (-0.35), indicating that as 'age' and 'oldpeak' increase, 'thalach' tends to decrease.
'trestbps' and 'chol' show very weak or no correlation with 'thalach' and 'oldpeak'.

```{r}
library(gridExtra)
library(ggplot2)

# Define colors for "NO" and "YES"
colors <- c("NO" = "green", "YES" = "red")

grid.arrange(
  ggplot(data2, aes(x = sex, fill = target)) +
    geom_bar(position = "fill") +
    scale_fill_manual(values = colors),

  ggplot(data2, aes(x = fbs, fill = target)) +
    geom_bar(position = "fill") +
    scale_fill_manual(values = colors),

  ggplot(data2, aes(x = exang, fill = target)) +
    geom_bar(position = "fill") +
    scale_fill_manual(values = colors),
  
  nrow = 3
)
```


```{r}
grid.arrange(
  ggplot(data2, aes(x = cp, fill = target))+
  geom_bar(position = "fill")+ theme(axis.text.x = element_text(angle = 90, hjust = 1)),
  
  ggplot(data2, aes(x = restecg, fill = target))+
  geom_bar(position = "fill")+ theme(axis.text.x = element_text(angle = 90, hjust = 1)), ncol = 2
)
```


```{r}
ggplot(data2, aes(x = age, y = trestbps, color = target, shape = target)) +
  geom_point() +
  geom_smooth(method = "auto", se = FALSE)
```

Data points are plotted on the graph and are represented by two different shapes: triangles and dots.

The legend in the top right corner shows that the triangles represent the category 'YES' and the dots represent 'NO,' which could be indicative of a binary target variable, like the presence or absence of a condition in a medical study.

There are two fitted lines, one for each category ('YES' and 'NO'), that suggest a trend or pattern within each group as age increases.

The trend lines indicate that for the 'YES' category, the measurement 'trestbps' tends to increase until about age 55 and then slightly decreases. For the 'NO' category, the measurement 'trestbps' remains relatively stable across ages, with a gentle rise and then a decline after age 60.

```{r}
ggplot(data2, aes(x = age, y = chol, color = target, shape = target))+
  geom_point()+
  geom_smooth(se = FALSE)
```

The legend in the top right corner indicates that triangles represent the category 'YES' and dots represent 'NO,' suggesting a binary classification similar to the previous graph.

There are two fitted lines that show the trend in cholesterol levels for each category across different ages. For the 'YES' category, the line indicates a gradual increase in cholesterol levels with age until about 55, then the trend levels off. For the 'NO' category, the line shows a more consistent increase in cholesterol levels with age.

The spread of the data points suggests there is a wide variation in cholesterol levels among individuals in both categories, and the trend lines are a simplified representation of this variation.

```{r}
ggplot(data2, aes(x = age, y = thalach, color = target, shape = target))+
  geom_point()+
  geom_smooth(se = FALSE)
```

```{r}
ggplot(data2, aes(x = age, y = oldpeak, color = target, shape = target))+
  geom_point()+
  geom_smooth(se = FALSE)
```

Here are some insights derived from the exploratory data analysis (EDA) performed on the dataset:

1. Data Overview:
   - The dataset contains information related to heart disease, with various demographic and clinical variables such as age, sex, chest pain type, blood pressure, cholesterol levels, etc.

2. Missing Values:
   - Visualization of missing values using `vis_miss()` shows the presence of missing data in the dataset.

3. Data Transformation:
   - The dataset was transformed by recoding certain variables and converting them into factors or changing their labels for better interpretability and analysis.

4. Summary Statistics:
   - Summary statistics provide an overview of the distribution of variables in the dataset.

5. Boxplots:
   - Boxplots show the distribution of variables across different categories, such as target variable (heart disease) and other categorical variables.

6. Bar Plot:
   - A bar plot visualizes the distribution of the target variable (heart disease) in the dataset.

7. Proportion of Heart Disease by Age Group:
   - Analysis shows the proportion of heart disease across different age groups, providing insights into age-related trends.

8. Proportion of Heart Disease by Chest Pain Type:
   - A bar plot illustrates the proportion of heart disease based on different types of chest pain, revealing potential associations between chest pain and heart disease.

9. Distribution of Population by Age and Sex:
   - Histograms display the distribution of age for male and female populations separately.

10. Density Plots:
    - Density plots visualize the distribution of numeric variables by the target variable (heart disease), providing insights into their distributions.

11. Correlation Analysis:
    - Correlation matrix and heatmap visualize the relationships between variables, helping identify potential correlations among them.

12. Comparison of Heart Disease Across Different Variables:
    - Various plots compare heart disease across different categorical and numerical variables, such as blood pressure, cholesterol levels, etc., providing insights into potential relationships.

#PCA(Principal_Component_Analysis)
```{r}
# prcomp is the principal component function
pca <- prcomp(data2[,10:14], scale = TRUE)  
pca

```

This output represents the results of principal component analysis (PCA) applied to a dataset with five variables (age, trestbps, chol, thalach, oldpeak). The standard deviations indicate the variability explained by each principal component (PC), with PC1 explaining the most variability and PC5 the least. The rotation matrix shows the correlations between the original variables and the principal components. For example, in PC1, age and thalach have relatively high positive correlations, while trestbps and chol have lower positive correlations. This indicates how much each original variable contributes to each principal component.

```{r}
# Variance of principal components
variance <- pca$sdev^2

# Proportion of variance explained by each principal component
variance_proportion <- variance / sum(variance)

# Print variance and proportion of variance explained
print(variance)
```

```{r}
summary(pca)

```

This result summarizes the importance of each principal component (PC) in a principal component analysis (PCA). The "Standard deviation" column represents the variability explained by each PC, with PC1 explaining the most and PC5 the least. "Proportion of Variance" indicates the proportion of total variance explained by each PC, with PC1 contributing the most variance and PC5 the least. "Cumulative Proportion" shows the cumulative proportion of variance explained by the PCs, indicating that the first PC alone explains 36.07% of the variance, while all five PCs combined explain 100% of the variance in the dataset.

```{r}
pca$sdev
```

```{r}
#Eigen vectors
pca$rotation
```

```{r}
#std deviation and mean of variables
pca$center
pca$scale
```

```{r}
# Principal component scores (showing only first 5 rows)
head(pca$x, 5)
```

```{r}
screeplot(pca)
```

```{r}
library(factoextra)
fviz_screeplot(pca)
```

# In order to break into pieces each variable contribution on each Principal Component we can use fviz_contrib function.
```{r}
var <- get_pca_var(pca)
a<-fviz_contrib(pca, "var",axes = 1)
b<-fviz_contrib(pca, "var",axes = 2)
grid.arrange(a,b,top='Contribution to the Principal Components')
```

In the first graph, "Contribution of variables to Dim-1," we see that several variables contribute to the first principal component, with their contributions shown in percentages. The variables are labeled along the x-axis (though the text is slightly obscured, possibly due to resolution issues), and the y-axis shows the percentage contribution. A dashed red line appears to indicate an average level of contribution across the variables. The first variable on the x-axis appears to contribute around 30%, which is the highest among all the variables shown.

In the second graph, "Contribution of variables to Dim-2," we see a similar setup with the same variables contributing to the second principal component. Again, the contributions are shown in percentages, and the same dashed red line indicates the average contribution level. In this case, the first variable on the x-axis also contributes the highest percentage to Dim-2, which is slightly over 30%.

# Print variance and proportion_variance
```{r}
# Variance of the first two principal components
variance <- (pca$sdev[1:2])^2

# Proportion of variance explained by the first two principal components
variance_proportion <- variance / sum(variance)

# Print variance and proportion of variance explained
print(variance)
print(variance_proportion)
```
Inference
First 2 Principal components:
1st PC: 62% 2nd PC: 37%

# Create a bar plot to visualize the variance of the first two principal components
```{r}
barplot(variance, 
        main = "Variance of First Two Principal Components",
        xlab = "Principal Component",
        ylab = "Variance",
        col = c("skyblue", "lightgreen"),  # Adjusted color palette
        border = NA,  # Remove border
        names.arg = NULL)  # Remove labels for principal components
```

#PCA Biplot
```{r}
fviz_pca_var(pca, col.var = "steelblue")

# Compute the biplot
biplot(pca, scale = 0)

# Add labels to the data points if needed
text(pca$x[, 1], pca$x[, 2], labels = rownames(data2), pos = 3, col = "blue")
```

From the plot, we can infer that:

Dimension 1 (Dim1):

"thalach" (maximum heart rate achieved): This variable has a strong positive loading on Dimension 1, suggesting it is an important variable in this component. In general, a higher maximum heart rate achieved during stress testing could be indicative of healthier heart function. However, in the context of predicting heart disease, the relationship might not be straightforward. Individuals with certain types of heart disease may also have a blunted heart rate response to exercise.

"trestbps" (resting blood pressure) and "chol" (cholesterol): These variables have a moderate negative and positive loading, respectively, on Dimension 1. High resting blood pressure and high cholesterol levels are known risk factors for heart disease. However, their influence on this component is not as strong as "thalach," suggesting that in this particular analysis, they may be less significant in explaining the variance captured by Dimension 1.

Dimension 2 (Dim2):

"oldpeak" (ST depression induced by exercise relative to rest): This variable has a strong positive loading on Dimension 2. ST depression is a sign of ischemia and can be an important predictor of coronary artery disease. Therefore, this variable might be relevant in predicting heart disease.

"age": This variable has a slight negative loading on Dimension 2. Age is a well-known risk factor for heart disease, with risk increasing as age increases. The direction and length of the vector for "age" suggest that it does have an influence on Dimension 2, but it might be explained in combination with other variables.

```{r}
fviz_pca(pca)
```

# Plot the first vs. second principal component
```{r}
# Define colors based on the first variable 
colors <- c(rep("blue", 100), rep("red", 100))  # Adjust as per your actual data

# Plot the first vs. second principal component with dots
plot(x = pca$x[, 1], y = pca$x[, 2], 
     pch = 16,  # Use a solid dot for better visualization
     col = colors,  # Use the defined colors
     xlab = "PC1", ylab = "PC2", main = "First vs. Second PC",
     cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8)
```

Most of the patients with heart disease (the red ones), are predicted to have a high probability of having heart disease and most of the patients without heart disease (the blue ones) are predicted to have a low probability of having a heart disease.
Thus logistic regression has done a pretty good job. 

#Cluster_Analysis
#K-Means Clustering
```{r}
uns_df <- scale(data2[,10:14])
head(as_tibble(uns_df))
```

```{r}
# Calculate distances between observations using the standardized data
distance <- dist(uns_df)

# Visualize the distance matrix
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

```{r}
k2 <- kmeans(uns_df, 
             center = 2,
             nstart = 25  )
str(k2)
```

The output is from a k-means clustering analysis performed on a dataset with 1025 observations and 5 variables (age, trestbps, chol, thalach, and oldpeak). Two clusters were identified, labeled 1 and 2, with respective cluster centers indicating the average values of each variable within each cluster. The total sum of squares (totss) represents the total variability in the data, while withinss and betweenss denote the variability within and between clusters, respectively. The size of each cluster indicates the number of observations assigned to it. The iter parameter indicates the number of iterations performed during the clustering process, and ifault suggests no errors occurred during execution.

```{r}
k2
```

```{r}
fviz_cluster(k2, data = uns_df)
```

The image displays a scatter plot titled "Cluster plot," which is used to visualize the clustering of data points in a two-dimensional space. The axes are labeled "Dim1 (36.1%)" and "Dim2 (25.1%)" which likely represent the first two principal components from a Principal Component Analysis (PCA). The percentages indicate how much of the variance in the data is explained by each principal component.

In the plot, there are two distinct clusters of data points, each highlighted with a different color and labeled as "1" and "2" in the legend under the label "cluster." Cluster 1 is colored red and Cluster 2 is colored blue. These clusters are also enclosed within their respective convex hulls, which are the smallest convex polygons that can contain all the points of a cluster. This visual aid helps to emphasize the separation between the clusters.

```{r}
k3 <- kmeans(uns_df, centers = 3, nstart = 25)
k4 <- kmeans(uns_df, centers = 4, nstart = 25)
k5 <- kmeans(uns_df, centers = 5, nstart = 25)

p1 <- fviz_cluster(k2, geom = "point", data = uns_df)+
  ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = uns_df)+
  ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = uns_df)+
  ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = uns_df)+
  ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1,p2,p3,p2, nrow = 2)

```

#Optimum cluster numbers
```{r}
# Elbow Method
set.seed(123)
fviz_nbclust(uns_df, kmeans, method = "wss")
```

In this plot, you can observe that as the number of clusters increases, the total WSS decreases. This is expected because when you increase the number of clusters, the clusters themselves become smaller and the points are closer to their respective centroids.

To find the optimal number of clusters, you look for the "elbow" point in the plot, which is the point after which the rate of decrease of WSS slows down significantly. This point represents a balance between minimizing the WSS and having a smaller number of clusters. In this plot, the elbow seems to appear around the 3 or 4 cluster mark, suggesting that 3 or 4 might be the optimal number of clusters for this particular dataset.


```{r}
# Average Silouette Method
fviz_nbclust(uns_df, kmeans, method = "silhouette")
```

#Gap_ Statistics
```{r}
library(cluster)
set.seed(123)

gap_stat <- clusGap(uns_df, FUN = kmeans, nstart = 25, K.max = 10, B = 50)

print(gap_stat, method = "firstmax")

fviz_gap_stat(gap_stat)
```

```{r}
set.seed(123)

final <- kmeans(uns_df, 2, nstart = 25)
final
```

# Descriptive Statistics for Clusters
```{r}
data2[,10:14] %>% 
  mutate(Cluster = final$cluster) %>% 
  group_by(Cluster) %>% 
  summarise_all("mean")

```

# HIERARCHICAL CLUSTERING
```{r}
xquant <- data2[10:14] # Numeric variables
xqual <- data2[2:9]    # Categorical variables
tree <- hclustvar(xquant, xqual)
plot(tree, cex = 0.6)
rect.hclust(tree, k = 4, border = 2:4)
```

In the dendrogram shown, you can see several clusters being merged at different heights. The decision on the number of clusters can be made by cutting the dendrogram at a certain height, which will define clusters as the number of vertical lines that are intersected by the horizontal line.

```{r}
stab <- stability(tree, B=50) # "B=50" refers to the number of bootstrap samples to use in the estimation.

d <- daisy(data2[-1], metric="gower")

fit <- hclust(d=d, method="complete")    # Also try: method="ward.D"   

plot(fit, cex = 0.6)
rect.hclust(fit, k = 4, border = 2:4)
```

```{r}
kfit <- kmeans(d, 4)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

# Plot the first two principal components with cluster assignments
```{r}
# Perform PCA
pc <- prcomp(uns_df)

# Extract the first two principal components
pc_first_two <- pc$x[, 1:2]

# Perform K-means clustering on the first two principal components
set.seed(12)  # For reproducibility
k <- 2  # Number of clusters
km_clusters <- kmeans(pc_first_two, centers = k)

# Plot the first two principal components with cluster assignments
plot(pc_first_two, col = km_clusters$cluster, 
     main = "First Two Principal Components with Cluster Assignments", 
     xlab = "", ylab = "", pch = 20)
```

The plot displays a scatterplot of data points reduced to two dimensions using Principal Component Analysis (PCA), where the first two principal components are plotted along the x and y axes, respectively. The data points are colored to represent two distinct clusters, as identified by a clustering algorithm, with black and red dots indicating the separate groupings. The plot's purpose is to illustrate the variance captured by PCA and the natural groupings within the data as determined by the clustering, enabling visualization and analysis of patterns within a high-dimensional dataset in a simplified two-dimensional space.


```{r}
library(NbClust)
res.nbclust <- data %>% scale() %>% NbClust(distance = "euclidean", min.nc = 2, max.nc = 10, method = "complete", index ="all")
```

#Factor_Analysis
```{r}
# Load necessary library
library(psych)

# Subset the dataset to include only columns 10 to 14
subset_data <- data2[, 10:14]

# Perform Factor Analysis
fit.pc <- principal(subset_data, nfactors = 3, rotate = "varimax")

# View factor analysis results
fit.pc

```

Variables (Rows): The rows represent different variables included in the factor analysis. These are 'age', 'trestbps' (resting blood pressure), 'chol' (cholesterol), 'thalach' (maximum heart rate achieved), and 'oldpeak' (ST depression induced by exercise relative to rest).

Factors (RC1, RC2, RC3): The columns labeled RC1, RC2, and RC3 represent the factor loadings on three extracted factors. Factor loadings can be interpreted similarly to correlation coefficients between the observed variables and the latent factors. For example, 'age' has a loading of 0.58 on RC1, indicating a moderate positive association with the first factor.

Communalities (h2): This column represents the communalities (h2), which are estimates of the proportion of each variable's variance that is accounted for by the common factors. For instance, the communality for 'age' is 0.6101805, suggesting that approximately 61% of the variance in age can be explained by the three factors together.

Uniqueness (u2): This column shows the uniqueness (u2), which is the proportion of variance that is unique to the variable and not shared with other variables in the analysis. It is the variance that is not explained by the extracted factors. For 'age', the uniqueness is 0.39881951, meaning around 39.9% of the variance in age is unique to it.

Complexity (com): The last column represents the complexity (com) of the variables. Complexity indicates how many factors significantly load on a variable. A complexity value close to 1 suggests that the variable is primarily associated with one factor, while a higher value indicates that the variable is complexly determined by multiple factors. For example, 'age' has a complexity of 2.267834, indicating that it’s influenced by more than one factor.

#Factor Loadings
```{r}
round(fit.pc$values, 3)
fit.pc$loadings
```

The output represents the results of a factor analysis or principal component analysis (PCA). The first line provides the standard deviations of the principal components, indicating their importance in explaining the variance within the data. The "Loadings" section displays the correlation coefficients between the original variables and the extracted components. For example, the loading of age on RC1 is 0.581, indicating a strong positive correlation. The "SS loadings" section shows the sum of squares loadings for each component, representing the proportion of variance explained by each component. In this case, RC1 explains 31.4% of the total variance, followed by RC2 and RC3. The "Proportion Var" and "Cumulative Var" sections provide the proportion and cumulative proportion of variance explained by each component, respectively.

# Communalities
```{r}
fit.pc$communality
```

```{r}
# Rotated factor scores, Notice the columns ordering: RC1, RC3, RC2 and RC4
fit.pc$scores

```

# Play with FA utilities
```{r}
fa.parallel(data2[, 10:14]) # See factor recommendation
```

In the plot, there are six lines representing different data:

"PC Actual Data" (marked with 'X') shows the eigenvalues for each principal component from the actual dataset.

"PC Simulated Data" (dotted line) represents the average eigenvalues for each component from the simulated datasets.

"PC Resampled Data" (dashed line) represents the eigenvalues from a resampling of the actual data (e.g., bootstrapping).

"FA Actual Data" (marked with 'Δ') shows the eigenvalues for each factor from the actual dataset using factor analysis.

"FA Simulated Data" (dotted line) represents the average eigenvalues for each factor from the simulated datasets using factor analysis.

"FA Resampled Data" (dashed line) represents the eigenvalues from a resampling of the actual data using factor analysis.

# See Correlations within Factors
```{r}
fa.plot(fit.pc) 
```

# Visualize the relationship
```{r}
fa.diagram(fit.pc) 
```

Interpreting the plot:

'thalach' has a strong negative correlation with RC1, as indicated by the loading score of -0.9 and the dashed line.

'oldpeak' has a somewhat strong positive correlation with RC1, with a loading score of 0.7.

'age' has a moderate positive correlation with RC1, indicated by the loading score of 0.6.

'chol' has a strong positive correlation with RC2, with a loading score of 0.9.

'trestbps' has a perfect positive correlation with RC3, as the loading score is 1.

RC1 might represent some form of heart health factor, given that 'thalach' (maximum heart rate achieved) is negatively correlated (as heart rate might decrease with certain heart conditions) and 'oldpeak' (ST depression induced by exercise relative to rest) is positively correlated with it, as higher values of ST depression can be associated with heart issues. RC2 and RC3 could represent other underlying factors or components that describe different dimensions of variation in the data.

# See Factor recommendations for a simple structure
```{r}
vss(data2[, 10:14]) 
```

# Computing Correlation Matrix
```{r}
corrm.data <- cor(data2[, 10:14])
corrm.data
```

This is a correlation matrix showing the pairwise correlations between the variables: age, trestbps (resting blood pressure), chol (serum cholesterol), thalach (maximum heart rate achieved), and oldpeak (ST depression induced by exercise relative to rest). Each cell in the matrix represents the correlation coefficient between two variables. For example, the correlation between age and trestbps is 0.271, indicating a weak positive correlation, while the correlation between thalach and oldpeak is -0.350, indicating a moderate negative correlation. This matrix helps understand how each variable is related to the others in the dataset, providing insights into potential relationships and dependencies between variables.


```{r}
plot(corrm.data)
```

#importance of each principal component
```{r}
data2_pca <- prcomp(data2[, 10:14], scale = TRUE)
summary(data2_pca)
```
This result summarizes the importance of each principal component (PC) extracted from a dimensionality reduction technique like Principal Component Analysis (PCA).

Standard Deviation: It represents the variability of the data captured by each principal component. Larger standard deviations indicate that the corresponding PCs explain more variance in the original data.
Proportion of Variance: It shows the proportion of the total variance in the data explained by each principal component. For example, PC1 explains 36.07% of the total variance, PC2 explains 21.51%, and so on.
Cumulative Proportion: It represents the cumulative proportion of variance explained by the principal components. In this case, the first PC alone explains 36.07% of the variance, while the first two PCs combined explain 57.58%, and so on.
These values help in understanding the relative importance of each principal component in capturing the variability of the original dataset.

#Plotting Variances of PCs
```{r}
plot(data2_pca)
```

```{r}
# A table containing eigenvalues and %'s accounted, follows. Eigenvalues are the sdev^2
(eigen_data2 <- round(data2_pca$sdev^2,3))

```

```{r}
round(fit.pc$values, 3)
```

#Eigen values
```{r}
eigen_data2 <- data2_pca$sdev^2
names(eigen_data2) <- paste("PC", 1:5, sep = "")
eigen_data2
```

#Sum of eigen values
```{r}
sumlambdas <- sum(eigen_data2)
sumlambdas
```
#calculates the proportion of variance
```{r}
propvar <- round(eigen_data2/sumlambdas,2)
propvar
```


```{r}
cumvar_data2 <- cumsum(propvar)
cumvar_data2
```


```{r}
matlambdas <- rbind(eigen_data2,propvar,cumvar_data2)
matlambdas
```

```{r}
rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
rownames(matlambdas)
```

```{r}
eigvec.data2 <- data2_pca$rotation
print(data2_pca)
```
This output is from a principal component analysis (PCA) of a dataset. The first section provides the standard deviations of the principal components, indicating their respective magnitudes of variability. The second section, the rotation matrix, displays the loadings of each original variable on each principal component. Each row represents an original variable, while each column corresponds to a principal component. The values in the matrix indicate the correlation between the original variables and the principal components.

# Taking the first four PCs to generate linear combinations for all the variables with four factors
```{r}
pcafactors.data2 <- eigvec.data2[,1:4]
pcafactors.data2
```

# Multiplying each column of the eigenvector’s matrix by the square-root of the corresponding eigenvalue in order to get the factor loadings
```{r}
unrot.fact.data2 <- sweep(pcafactors.data2,MARGIN=2,data2_pca$sdev[1:4],`*`)
unrot.fact.data2
```

# Computing communalities
```{r}
communalities.data2 <- rowSums(unrot.fact.data2^2)
communalities.data2
```

# Performing the varimax rotation. The default in the varimax function is norm=TRUE thus, Kaiser normalization is carried out
```{r}
rot.fact.data2 <- varimax(unrot.fact.data2)
rot.fact.data2
```

This output represents the results of a factor analysis, specifically the loadings matrix and rotation matrix. The loadings matrix displays the correlations between the original variables and the extracted factors (PCs). Each row corresponds to an original variable, while each column represents a factor. For instance, in the first column (PC1), 'age' has a high loading (0.832), indicating a strong correlation with PC1. In contrast, 'trestbps' has a high loading (0.962) in PC2, suggesting it contributes most to the variance explained by PC2. The rotation matrix displays the rotated loadings after applying an orthogonal transformation to simplify interpretation. Each column represents a factor, and each row represents a variable. The values in the matrix indicate the correlation between the rotated factors and the original variables.

# The print method of varimax omits loadings less than abs(0.1). In order to display all the loadings, it is necessary to ask explicitly the contents of the object $loadings
```{r}
fact.load.data2 <- rot.fact.data2$loadings
fact.load.data2
```

This output presents the loadings and variance explained by each principal component (PC) obtained from a factor analysis or principal component analysis (PCA). The loadings represent the correlation between the original variables and the PCs. For instance, in PC1, 'age' has a loading of 0.832, indicating a strong positive correlation, while 'chol' has a loading of -0.988, suggesting a strong negative correlation. The "SS loadings" row shows the sum of squared loadings for each PC, indicating the amount of variance explained by each component. The "Proportion Var" row displays the proportion of total variance explained by each PC, while the "Cumulative Var" row shows the cumulative proportion of variance explained up to each PC.


```{r}
# Computing the rotated factor scores for the data. Notice that signs are reversed for factors F2, F3, and F4
scale.emp <- scale(data2[, 10:14])
factor_scores <- as.matrix(scale.emp) %*% fact.load.data2 %*% solve(t(fact.load.data2) %*% fact.load.data2)
factor_scores
```

#Multiple_Rgression
```{r}

correlation_matrix <- cor(data2[c("age", "trestbps", "chol", "thalach", "oldpeak")])

# Display correlation matrix
print(correlation_matrix)
```

This table represents the correlation matrix between the variables 'age', 'trestbps', 'chol', 'thalach', and 'oldpeak'. Each cell in the table contains the correlation coefficient between the corresponding pair of variables.

The correlation coefficient between 'age' and 'trestbps' is approximately 0.271, indicating a weak positive correlation.
The correlation coefficient between 'age' and 'thalach' is approximately -0.390, indicating a moderate negative correlation.
The correlation coefficient between 'chol' and 'thalach' is approximately -0.022, indicating a very weak negative correlation.
The correlation coefficient between 'thalach' and 'oldpeak' is approximately -0.350, indicating a moderate negative correlation.

```{r}
df <- as.data.frame(data2)

# Create a linear regression model
model <- lm(thalach ~ age , data = df)

# Display the summary of the regression model
summary(model)
```

Residuals: These are the differences between the observed and predicted values of the target variable. They show the variability that is not explained by the model. The minimum residual is -65.680, and the maximum is 45.456.
Coefficients: The 'Estimate' column provides the estimated coefficients of the linear regression model. The intercept is 202.9793, indicating the expected value of 'thalach' when 'age' is 0. The coefficient for 'age' is -0.9896, indicating that for each one-unit increase in 'age', the 'thalach' decreases by approximately 0.9896 units.


```{r}
df <- as.data.frame(data2)

# Create a multiple linear regression model
fit <- lm(thalach ~ age + trestbps + chol + oldpeak, data = df)

# Display the summary of the regression model
summary(fit)
```

```{r}
plot(fit)
```


In the resudual vs fitted plot , it is almost same. The dotted line is nearby red line. So the data is normally distributed. 

The Q-Q plot indicates that the residuals of the model are not perfectly normal, particularly at the tails, the central portion of the data follows the expected pattern closely. This might be acceptable in many practical situations,

This is the homoscedasticity , the values are equally scattered.


```{r}
coefficients(fit)
```

```{r}
confint(fit,level=0.95)
```

```{r}
# Create a dataframe with fitted values and residuals
residuals_df <- data.frame(
  fitted_values = fitted(fit),
  residuals = residuals(fit)
)

# Load the ggplot2 library if not already loaded
library(ggplot2)

# Plot residuals against fitted values to check for homoscedasticity
plot_resid_fitted <- ggplot(residuals_df, aes(x = fitted_values, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(x = "Fitted Values", y = "Residuals",
       title = "Residuals vs Fitted Values Plot") +
  theme_minimal()

print(plot_resid_fitted)
```

In this plot:

The horizontal axis is labeled "Fitted Values," which represent the values predicted by the regression model.

The vertical axis is labeled "Residuals," which are the differences between the observed values and the fitted (predicted) values for each observation in the dataset.

The points on the plot are the individual observations. Each point represents one observation's residual corresponding to its fitted value.

A horizontal dashed line at zero on the residuals axis represents the ideal case where the residuals would be if the model perfectly predicted the observed values.

#Prediction
```{r}
new_data <- data.frame(predict.lm(fit, data.frame( age= 70, trestbps=145,  chol=300 , oldpeak=3)))
new_data
```

# Model Accuracy
```{r}
rsquared <- summary(fit)$r.squared
cat("R-squared:", rsquared, "\n")
```

```{r}
adjusted_rsquared <- summary(fit)$adj.r.squared
cat("Adjusted R-squared:", adjusted_rsquared, "\n")
```

#Nonlinearity
# component + residual plot
```{r}
library(car)
crPlots(fit)
```

The top left plot shows the relationship between "age" and the dependent variable. The solid purple line represents the fitted values, showing a general negative trend, indicating that as age increases, the dependent variable tends to decrease. The blue dashed line is the zero line, which helps to assess the distribution of residuals.

The top right plot is for "trestbps" (which might stand for resting blood pressure) against the dependent variable. Again, there's a general negative trend indicated by the purple line.

The bottom left plot shows the relationship for "chol" (possibly cholesterol levels) with the dependent variable. This plot reveals a slight positive trend, suggesting that higher cholesterol levels might be associated with an increase in the dependent variable.

The bottom right plot represents the variable "oldpeak" (which might relate to ST depression induced by exercise relative to rest) against the dependent variable. There is a noticeable negative trend, as the purple line slopes downwards.

# plot studentized residuals vs. fitted values
```{r}
library(car)
spreadLevelPlot(fit)
```

In the plot:

The x-axis represents the fitted values (the predicted values from the regression model).
The y-axis shows the absolute standardized residuals (the absolute value of the residuals, which are the differences between the observed values and the fitted values, standardized by dividing by an estimate of the standard deviation of the residuals).


#LDA(Linear_Discrimination_Analysis)

```{r}
Ldata <- as.matrix(data2[, c(10:14)])
row.names(Ldata) <- row.names(data2)
Lraw <- cbind(Ldata, as.numeric(as.factor(data2$target)) - 1)
colnames(Lraw)[ncol(Lraw)] <- "diagnosis"
smp_size_raw <- floor(0.75 * nrow(Lraw))
train_ind_raw <- sample(nrow(Lraw), size = smp_size_raw)
train_raw.df <- as.data.frame(Lraw[train_ind_raw, ])
test_raw.df <- as.data.frame(Lraw[-train_ind_raw, ])
Lraw.lda <- lda(formula = diagnosis ~ ., data = train_raw.df)
Lraw.lda
```

Prior probabilities: These are the probabilities of belonging to each group (0 and 1) before considering any information from the predictor variables. Here, the prior probability of group 0 is 0.4974, and the prior probability of group 1 is 0.5026.
Group means: These are the mean values of each predictor variable for each group. For example, for group 0, the mean age is 56.62, mean 'trestbps' is 133.71, mean 'chol' is 250.87, mean 'thalach' is 139.46, and mean 'oldpeak' is 1.58.
Coefficients of linear discriminants: These coefficients represent the weights assigned to each predictor variable in the linear combination that maximizes the separation between the groups. In this case, the linear discriminant function (LD1) is constructed using these coefficients. For example, a one-unit increase in 'thalach' corresponds to an increase of 0.0307 units in LD1, while a one-unit increase in 'oldpeak' corresponds to a decrease of 0.5746 units in LD1.

#Summary
```{r}
summary(Lraw.lda)
```

```{r}
print(Lraw.lda)
```

Prior probabilities: These are the probabilities of belonging to each group (0 and 1) before considering any predictor variables.
Group means: These represent the average values of predictor variables for each group. For instance, group 0 has a mean age of 56.62, mean 'trestbps' of 133.71, mean 'chol' of 250.87, mean 'thalach' of 139.46, and mean 'oldpeak' of 1.58, while group 1 has lower means for all variables.
Coefficients of linear discriminants: These coefficients are used to form a linear combination of predictor variables that best separates the groups. For example, 'thalach' has a positive coefficient, indicating that higher values of 'thalach' contribute to the prediction of group 1, while 'oldpeak' has a negative coefficient, suggesting that higher values of 'oldpeak' contribute to the prediction of group 0.

```{r}
plot(Lraw.lda)
```

These histograms represent the distributions of Linear Discriminant Analysis (LDA) scores for two different groups or classes in a dataset. LDA is a technique used in statistics and machine learning for pattern classification and dimensionality reduction. It projects the data onto a lower-dimensional space where the classes are most separable.

In the context of LDA:

Classes (groups) are typically predefined based on the labels in the dataset.

LDA tries to find a linear combination of features that best separates the different classes.

The resulting histograms show the distribution of the projected data along the LDA component axis for each class.

For "group 0," the histogram shows a fairly normal distribution of LDA scores centered around zero. For "group 1," the distribution is also roughly normal but appears to be slightly skewed towards the right.

```{r}
# Make predictions on the test data
Lraw.lda.predict <- predict(Lraw.lda, newdata = test_raw.df)

# Extract predicted classes
Lraw.lda.predict$class
```

```{r}
Lraw.lda.predict$x
```

#Accuracy
```{r}
# Get the predicted classes
predicted_classes <- predict(Lraw.lda, newdata = test_raw.df)$class

# Calculate accuracy
accuracy <- mean(predicted_classes == test_raw.df$diagnosis)
accuracy
```
#ROC Plot
```{r}
# Get the posteriors as a dataframe
Lraw.lda.predict.posteriors <- as.data.frame(predict(Lraw.lda, newdata = test_raw.df)$posterior)

# Calculate the performance metrics
pred <- prediction(Lraw.lda.predict.posteriors[, 2], test_raw.df$diagnosis)
roc.perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")@y.values

# Plot the ROC curve
plot(roc.perf)
abline(a = 0, b = 1)
text(x = 0.25, y = 0.65, paste("AUC = ", round(auc.train[[1]], 3), sep = ""))
```

In this ROC curve:

The x-axis represents the False Positive Rate (FPR): the proportion of negative instances that are incorrectly classified as positive.
The y-axis represents the True Positive Rate (TPR): the proportion of positive instances that are correctly classified.
The plot also includes a diagonal line, which represents the performance of a completely random classifier (AUC = 0.5). An ideal classifier would yield a point in the upper left corner of the ROC space, corresponding to a TPR of 1 (perfect sensitivity) and an FPR of 0 (perfect specificity).

The Area Under the Curve (AUC) value is a measure of the overall performance of the classifier and is shown in the plot as "AUC = 0.821". An AUC of 1 represents a perfect classifier, and an AUC of 0.5 represents a worthless classifier. An AUC of 0.821 suggests that the classifier has a good discriminatory ability to distinguish between the positive class and the negative class.


#Logistic_Regression
```{r}
library(tidyverse)
library(readr)
library(ROCR)
library(PerformanceAnalytics)
library(e1071)
library(caret)
library(gbm)
library(corrplot)
library(ggcorrplot)
library(MASS)
library(rpart)
library(caTools)
library(naivebayes)
library(class)
library(ISLR)
library(glmnet)
library(Hmisc)
library(funModeling)
library(pROC)
library(randomForest)
library(klaR)
library(scales)
library(cluster)
library(factoextra)
library(DataExplorer)
library(ClustOfVar)
library(GGally)
library(dplyr)
library(visdat)
```


```{r}

file_path <- "D:/Multivariate Analysis/Final_Project/heart.csv"
data <- read.csv(file_path)
head(data)
```



#Transformation
```{r}

data2 <- data %>% 
  mutate(sex = if_else(sex == 1, "MALE", "FEMALE"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "YES" ,"NO"),
         cp = if_else(cp == 1, "ATYPICAL ANGINA",
                      if_else(cp == 2, "NON-ANGINAL PAIN", "ASYMPTOMATIC")),
         restecg = if_else(restecg == 0, "NORMAL",
                           if_else(restecg == 1, "ABNORMALITY", "PROBABLE OR DEFINITE")),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal),
         target = if_else(target == 1, "UNHEALTHY", "HEALTHY")
         ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())

```



```{r}
str(data2)
```

# Now we see how many rows of data have NA values
```{r}
nrow(data2[is.na(data2$ca) | is.na(data2$thal), ])
```

There is no rows of data having NAs in them.

It is now necessary to ensure that samples of both healthy and diseased individuals are taken from each gender (male and female). We should presumably eliminate every female from the model if heart disease is limited to male samples.

```{r}
xtabs(~ target + sex, data=data2)
```
Healthy and Unhealthy patients are both represented by a lot of female and male samples.

For all 3 levels of chest pain reported by patients
```{r}
xtabs(~ target + cp, data=data2)
```



```{r}
xtabs(~ target + fbs, data=data2)
```


```{r}
xtabs(~ target + restecg, data=data2)
```


```{r}
xtabs(~ target + exang, data=data2)
```


```{r}
xtabs(~ target + slope, data=data2)
```


```{r}
xtabs(~ target + ca, data=data2)
```


```{r}
xtabs(~ target + thal, data=data2)
```

#Applying Rgression model
A simple model in which we will try to predict heart disease only using the gender of eah patient.
```{r}
logistic <- glm(target ~ sex, data = data2, family = "binomial")
summary(logistic)
```
Here the glm() function that performs Generalized Linear Models.
we used formula syntax(hd~sex) to predict heart disease(target).
Then we specify the data(data=data2) that we are using for this model.
In last, we specify that we want the binomial family of generalized linear models. This makes the glm() function do logistic regression, as apposed to some other type of generalized linear model.
we are storing the output from glm() in a variable called "logistic".
Then use the summary function to get details about the logistic regression.

summary output:
The first line has the original call to the glm() function.
Then it gives the summary of the deviance residuals.They look good since they are close to being centered on 0 and are roughly symmetrical.

Then we have the coefficients. They corresponding to the following model: 
Heart disease = 0.9662 -1.2859 x the patient is male.
The variable(the patient is male, is equal to 2 when the patient is female and 1 when the patient is male ).
Thus if are predicting heart disease for a female patient, we get the following equation:
Heart disease = 0.9662 -1.2859 x 2 = -1.6056.
Thus the log (odds) that a female has heart disease = -1.6056
If are predicting heart disease for a male patient, we get the following equation:
Heart disease = 0.9662 -1.2859 x 1 = -0.3197.
Since the first term(0.9662) is the log(odds) of a female having heart disease and the second term(-1.2859) indicates the increase in the log(odds) that a male has of having heart disease. In other words, the second term(-1.2859) is the log(odds ratio) of the odds that a male will have heart disease over the odds that a female will have heart disease.

The output std.error and z value shows how the Walds test was computed for both coefficients.

The p-values(Pr(>|z|)) are well below 0.05 and thus, the log(odds) and the log(odd ratio) are both statstically significant.

When we do "normal linear regression , we estimate both the mean and variance  from the data". In contrast, with logistic regression, we estimate the mean of the data, and the variance is derived from the mean. Since we are not estimating the variance from the data(instead of just deriving it from the mean), it is possible that the variance is underestimated.
If so, you can adjust the dispersion parameter in the summary command.

The null deviance and residuals deviance can be used to compare models , compute R^2 and an overall P-value.

Lastly, we have the number of Fisher Scoring iterations, which just tell us how quickly the glm()  function converged on the maximum likelihood estimates for the coefficients


```{r}
median(data2$age)
```

```{r}
logistic <- glm(target~ sex, data=data2, family= "binomial")
```
we have done a simple logistic regression using just one of the variables (sex) to predict heart disease, we can create a fancy model that uses all of the variables to predict heart disease.

```{r}
logistic <- glm(target~ ., data=data2, family= "binomial")
summary(logistic)
```
This formula syntax, target~., means that we want to model heart disease(target) using all of the remaining variables in our data.frame called "data2".

From the above result, we will just talk about a few of the coefficients:
We see that age is not a useful predictor because it has a large P-value. However the median age in our dataset was 56, so most of the folks were pretty old and that explains why its was not very useful.

The predictors with p-values less than 0.05 are: sexMALE, exangYES, cpNON-ANGINAL PAIN, ca1, ca2, ca3, thal1, thal2, age,
trestbps, chol, thalach, oldpeak.

On the bottom of the output, we see that the Residual Deviance and the AIC are much smaller for this fancy model than they were for the simple model, when we only used gender to predict heart disease.

If we want to calculate the  McFadden's Pseudo R^2 , we can pull the log -likelihood of the null model out of the logistic variable by getting the value for the null deviance and dividing by -2 and we can pull the log-likelihood for the fancy model out of the logistic variable by getting the value for the residual deviance and dividing by -2.

```{r}
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
 
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
```

Then we just do the math and ended up with a Pseudo R^2 = 0.54. This can ne interpreted as the overall effect size. And we can use the same log_likelihoods to calculate a P_valuefor that R^2 using a Chi-square distribution.

```{r}
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
```

In this case, the tiny P-value indicates that the R^2 value is not a result of random chance.

Lastly, we can draw a graph that shows the predicted probabilities that each patient has heart disease along with their actual heart disease status. 

```{r}
## Load necessary libraries
library(ggplot2)
library(cowplot)

## Create a data frame with predicted probabilities and actual heart disease status
predicted.data <- data.frame(
  probability.of.target = logistic$fitted.values,
  target = ifelse(data$target == 0, "Healthy", "Unhealthy")
)

## Order the predicted data by probability of target, and add a rank column (sort the data.frame from low probabilities to high probabilities)
predicted.data <- predicted.data[order(predicted.data$probability.of.target, decreasing = FALSE), ]
# Then we add a new column to the data.frame that is rank of each sample , from low probabability to high probability.
predicted.data$rank <- 1:nrow(predicted.data)

## Plot the predicted probabilities for each sample, color by actual heart disease status
ggplot(data = predicted.data, aes(x = rank, y = probability.of.target)) +
  geom_point(aes(color = target), alpha = 1, shape = 4, stroke = 2) +
  scale_color_manual(values = c("Healthy" = "blue", "Unhealthy" = "red")) +  ## Customize colors
  xlab("Index") +
  ylab("Predicted probability of heart disease") +
  theme_minimal()

## Save the plot as a PDF
ggsave("heart_disease_probabilities.pdf")
```

Most of the patients with heart disease (the red ones), are predicted to have a high probability of having heart disease and most of the patients without heart disease (the blue ones) are predicted to have a low probability of having a heart disease.
Thus logistic regression has done a pretty good job. 



