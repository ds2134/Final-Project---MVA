---
title: "Project_Final"
author: "Deviprasad Saka"
date: "2024-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset/data

#About Dataset

#Context
This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The "target" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.

#Content
Attribute Information:

age
sex
chest pain type (4 values)
resting blood pressure
serum cholestoral in mg/dl
fasting blood sugar > 120 mg/dl
resting electrocardiographic results (values 0,1,2)
maximum heart rate achieved
exercise induced angina
oldpeak = ST depression induced by exercise relative to rest
the slope of the peak exercise ST segment
number of major vessels (0-3) colored by flourosopy
thal: 0 = normal; 1 = fixed defect; 2 = reversable defect
The names and social security numbers of the patients were recently removed from the database, replaced with dummy values.

```{r}
library(tidyverse)
library(readr)
library(ROCR)
library(PerformanceAnalytics)
library(e1071)
library(caret)
library(gbm)
library(corrplot)
library(ggcorrplot)
library(MASS)
library(rpart)
library(caTools)
library(naivebayes)
library(class)
library(ISLR)
library(glmnet)
library(Hmisc)
library(funModeling)
library(pROC)
library(randomForest)
library(klaR)
library(scales)
library(cluster)
library(factoextra)
library(DataExplorer)
library(ClustOfVar)
library(GGally)
library(dplyr)
library(visdat)
library(naniar)
library(mice)
library(VIM)
```


(EDA) Exploratory Data Analysis: 

```{r}

file_path <- "D:/Multivariate Analysis/Final_Project/heart.csv"
data <- read.csv(file_path)
head(data)
```

#Missing Values
```{r}
#plot_missing(data)
vis_miss(data)

```




#Transformation
```{r}

data2 <- data %>% 
  mutate(sex = if_else(sex == 1, "MALE", "FEMALE"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "YES" ,"NO"),
         cp = if_else(cp == 1, "ATYPICAL ANGINA",
                      if_else(cp == 2, "NON-ANGINAL PAIN", "ASYMPTOMATIC")),
         restecg = if_else(restecg == 0, "NORMAL",
                           if_else(restecg == 1, "ABNORMALITY", "PROBABLE OR DEFINITE")),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal),
         target = if_else(target == 1, "YES", "NO")
         ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())

```


This code modifies the dataset `data` by recoding certain variables and converting them into factors or changing their labels. Here's what each line does:
The changes made are:
- Recoding binary variables into descriptive categories (e.g., `sex`, `fbs`, `exang`).
- Recoding categorical variables into more readable labels (e.g., `cp`, `restecg`).
- Converting certain variables into factors (e.g., `slope`, `ca`, `thal`).
- Converting the `target` variable into "YES" or "NO".
- Converting all character variables into factors.
- Rearranging columns so that `target`, `sex`, `fbs`, `exang`, `cp`, `restecg`, `slope`, `ca`, `thal`, and other variables are selected in that order.

#Summary Statistics
```{r}
summary(data2)

```


```{r}
# Define colors for boxplots
colors <- c("red", "blue", "green", "orange", "purple")

# Plot boxplots with different colors
boxplot(data2[, 10:14], col = colors)

```


#Data Visualization

# Bar plot for target (Heart disease)
```{r}
ggplot(data2, aes(x=data2$target, fill=data2$target)) + 
  geom_bar() +
  xlab("Heart Disease") +
  ylab("Count") +
  ggtitle("Analysis of Presence and Absence of Heart Disease") +
  scale_fill_discrete(name = "Heart Disease", labels = c("Absence", "Presence"))

```


```{r}
prop.table(table(data2$target))

```

# Counting the frequency of the values of the age
```{r}


data2 %>% 
  group_by(age) %>% 
  count() %>% 
  filter(n > 10) %>% 
  ggplot()+
  geom_col(aes(age, n), fill = "blue")+
  ggtitle("Age Analysis") +
  xlab("Age")  +
  ylab("AgeCount")
```



```{r}
library(ggplot2)
library(dplyr)

# Group the age columns of the analysis by age_group
data2$age_group <- cut(data2$age, breaks = seq(25, 75, 5))

# Calculate heart disease proportion for each age group
age_group <- data2 %>%
  group_by(age_group) %>%
  summarise(heart_dis_prop = round(sum(target == "YES") / n(), 3) * 100)  %>%
  na.omit()

# Plot the heart disease proportion by age group
ggplot(age_group, aes(x = age_group, y = heart_dis_prop, fill = age_group)) +
  geom_col(position = "dodge") + 
  labs(x = "Age Group", y = "Heart Disease Proportion", title = "Age Group Heart Disease Proportion")


```

```{r}
library(ggplot2)

# Modify the plot
ggplot(data2, aes(x = factor(cp), fill = factor(target))) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "Chest Pain Type", y = "Proportion", fill = "Target") +
  ggtitle("Proportion of Target by Chest Pain Type") +
  scale_fill_manual(values = c("NO" = "#4CAF50", "YES" = "#F44336"),
                    labels = c("No Heart Disease", "Heart Disease")) +
  theme_minimal()

```

#Distribution of Male and Female population across Age parameter
```{r}

library(ggplot2)

# Create separate plots for male and female populations
data2 %>%
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  xlab("Age") +
  ylab("Number") +
  facet_wrap(~ sex, nrow = 1) +  # Create separate plots for each sex
  theme_minimal()  # Use minimal theme for cleaner appearance



```

```{r}

# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Assuming your dataset is named data2
# Convert target variable to a factor with levels "NO" and "YES"
data2$target <- factor(data2$target, levels = c("NO", "YES"))

# Define a function to create density plots
plot_density <- function(data, var) {
  ggplot(data, aes(x = !!as.name(var), fill = target)) +
    geom_density(alpha = 0.5) +
    labs(title = paste("Density plot of", var, "by target")) +
    theme_minimal()
}

# Create density plots for all numeric variables
plots <- lapply(names(data2)[sapply(data2, is.numeric)], function(var) {
  plot_density(data2, var)
})

# Arrange plots in a grid layout
grid.arrange(grobs = plots, ncol = 3)


```



# Representation of Cholesterol level 
```{r}

library(ggplot2)

# Modify the plot
data2 %>%
  ggplot(aes(x = age, y = chol, color = sex, size = chol)) +
  geom_point(alpha = 0.7) +
  labs(x = "Age", y = "Cholesterol", color = "Sex") +  # Change axis and legend labels
  scale_color_manual(values = c("blue", "red"), labels = c("Female", "Male")) +  # Customize color scale
  scale_size_continuous(range = c(2, 6)) +  # Adjust point size range
  theme_minimal()  # Use minimal theme for cleaner appearance


```



# Comparison of Blood pressure across pain type 
```{r}

library(ggplot2)

# Modify the plot
data2 %>%
  ggplot(aes(x = sex, y = trestbps, fill = factor(cp))) +
  geom_boxplot() +
  scale_fill_brewer(palette = "Set1") +  # Use a color palette for pain types
  labs(x = "Sex", y = "Blood Pressure (BP)", fill = "Pain Type") +  # Change axis and legend labels
  theme_minimal() +  # Use minimal theme for a cleaner appearance
  facet_grid(~cp)  # Facet by pain type

```


```{r}
library(ggplot2)

# Modify the plot
data2 %>%
  ggplot(aes(x = sex, y = chol, fill = sex)) +
  geom_boxplot(color = "black", alpha = 0.7) +  # Add border and transparency to boxplots
  labs(x = "Sex", y = "Cholesterol", fill = "Sex") +  # Adjust axis and legend labels
  facet_grid(~ cp, scales = "free") +  # Allow scales to vary across facets
  scale_fill_manual(values = c("pink", "green"), labels = c("Female", "Male")) +  # Customize fill colors
  theme_minimal() +  # Use minimal theme for cleaner appearance
  theme(strip.text.x = element_text(size = 10), legend.position = "none")  # Adjust text size and remove legend


```


```{r}

library(ggplot2)

ggplot(data2) +
  geom_histogram(aes(x = oldpeak, y = ..density.., fill = target), 
                 position = "dodge", color = "black", alpha = 0.7, bins = 20) +
  geom_density(aes(x = oldpeak, fill = "Density"), alpha = 0.3, color = "red") +
  scale_fill_manual(values = c("NO" = "blue", "YES" = "green", "Density" = "orange")) +
  labs(x = "Oldpeak", y = "Density", fill = "") +  # Adjust axis labels and legend title
  theme_minimal()  # Use minimal theme for cleaner appearance


```




```{r}

# Calculate the correlation matrix
cor_heart <- cor(data2[, 10:14])

# Display the correlation matrix
cor_heart

# Plot the correlation matrix in a heatmap
corrplot(cor_heart, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45,
         addCoef.col = "black", number.cex = 0.7)


```




```{r}
library(gridExtra)
library(ggplot2)

# Define colors for "NO" and "YES"
colors <- c("NO" = "green", "YES" = "red")

grid.arrange(
  ggplot(data2, aes(x = sex, fill = target)) +
    geom_bar(position = "fill") +
    scale_fill_manual(values = colors),

  ggplot(data2, aes(x = fbs, fill = target)) +
    geom_bar(position = "fill") +
    scale_fill_manual(values = colors),

  ggplot(data2, aes(x = exang, fill = target)) +
    geom_bar(position = "fill") +
    scale_fill_manual(values = colors),
  
  nrow = 3
)


```


```{r}
grid.arrange(
  ggplot(data2, aes(x = cp, fill = target))+
  geom_bar(position = "fill")+ theme(axis.text.x = element_text(angle = 90, hjust = 1)),
  
  ggplot(data2, aes(x = restecg, fill = target))+
  geom_bar(position = "fill")+ theme(axis.text.x = element_text(angle = 90, hjust = 1)), ncol = 2
)

```


```{r}

ggplot(data2, aes(x = age, y = trestbps, color = target, shape = target)) +
  geom_point() +
  geom_smooth(method = "auto", se = FALSE)


```


```{r}

ggplot(data2, aes(x = age, y = chol, color = target, shape = target))+
  geom_point()+
  geom_smooth(se = FALSE)

```


```{r}

ggplot(data2, aes(x = age, y = thalach, color = target, shape = target))+
  geom_point()+
  geom_smooth(se = FALSE)

```


```{r}
ggplot(data2, aes(x = age, y = oldpeak, color = target, shape = target))+
  geom_point()+
  geom_smooth(se = FALSE)

```





Here are some insights derived from the exploratory data analysis (EDA) performed on the dataset:

1. Data Overview:
   - The dataset contains information related to heart disease, with various demographic and clinical variables such as age, sex, chest pain type, blood pressure, cholesterol levels, etc.

2. Missing Values:
   - Visualization of missing values using `vis_miss()` shows the presence of missing data in the dataset.

3. Data Transformation:
   - The dataset was transformed by recoding certain variables and converting them into factors or changing their labels for better interpretability and analysis.

4. Summary Statistics:
   - Summary statistics provide an overview of the distribution of variables in the dataset.

5. Boxplots:
   - Boxplots show the distribution of variables across different categories, such as target variable (heart disease) and other categorical variables.

6. Bar Plot:
   - A bar plot visualizes the distribution of the target variable (heart disease) in the dataset.

7. Proportion of Heart Disease by Age Group:
   - Analysis shows the proportion of heart disease across different age groups, providing insights into age-related trends.

8. Proportion of Heart Disease by Chest Pain Type:
   - A bar plot illustrates the proportion of heart disease based on different types of chest pain, revealing potential associations between chest pain and heart disease.

9. Distribution of Population by Age and Sex:
   - Histograms display the distribution of age for male and female populations separately.

10. Density Plots:
    - Density plots visualize the distribution of numeric variables by the target variable (heart disease), providing insights into their distributions.

11. Correlation Analysis:
    - Correlation matrix and heatmap visualize the relationships between variables, helping identify potential correlations among them.

12. Comparison of Heart Disease Across Different Variables:
    - Various plots compare heart disease across different categorical and numerical variables, such as blood pressure, cholesterol levels, etc., providing insights into potential relationships.




#PCA(Principal_Component_Analysis)
```{r}
# prcomp is the principal component function
pca <- prcomp(data2[,10:14], scale = TRUE)  
pca

```


```{r}
# Variance of principal components
variance <- pca$sdev^2

# Proportion of variance explained by each principal component
variance_proportion <- variance / sum(variance)

# Print variance and proportion of variance explained
print(variance)


```

```{r}
summary(pca)

```


```{r}
pca$sdev

```


```{r}
#Eigen vectors
pca$rotation

```


```{r}
#std deviation and mean of variables
pca$center
pca$scale

```


```{r}

# Principal component scores (showing only first 5 rows)
head(pca$x, 5)

```

```{r}

screeplot(pca)
```

```{r}

fviz_screeplot(pca)

```


# In order to break into pieces each variable contribution on each Principal Component we can use fviz_contrib function.
```{r}
var <- get_pca_var(pca)
a<-fviz_contrib(pca, "var",axes = 1)
b<-fviz_contrib(pca, "var",axes = 2)
grid.arrange(a,b,top='Contribution to the Principal Components')

```


# Print variance and proportion_variance
```{r}

# Variance of the first two principal components
variance <- (pca$sdev[1:2])^2

# Proportion of variance explained by the first two principal components
variance_proportion <- variance / sum(variance)

# Print variance and proportion of variance explained
print(variance)
print(variance_proportion)

```

Inference
First 2 Principal components:
1st PC: 62% 2nd PC: 37%

```{r}
# Create a bar plot to visualize the variance of the first two principal components
barplot(variance, 
        main = "Variance of First Two Principal Components",
        xlab = "Principal Component",
        ylab = "Variance",
        col = c("skyblue", "lightgreen"),  # Adjusted color palette
        border = NA,  # Remove border
        names.arg = NULL)  # Remove labels for principal components


```



#PCA Biplot
```{r}
fviz_pca_var(pca, col.var = "steelblue")

# Compute the biplot
biplot(pca, scale = 0)

# Add labels to the data points if needed
text(pca$x[, 1], pca$x[, 2], labels = rownames(data2), pos = 3, col = "blue")


```

From the plot, we can infer that:

Dimension 1 (Dim1):

"thalach" (maximum heart rate achieved): This variable has a strong positive loading on Dimension 1, suggesting it is an important variable in this component. In general, a higher maximum heart rate achieved during stress testing could be indicative of healthier heart function. However, in the context of predicting heart disease, the relationship might not be straightforward. Individuals with certain types of heart disease may also have a blunted heart rate response to exercise.

"trestbps" (resting blood pressure) and "chol" (cholesterol): These variables have a moderate negative and positive loading, respectively, on Dimension 1. High resting blood pressure and high cholesterol levels are known risk factors for heart disease. However, their influence on this component is not as strong as "thalach," suggesting that in this particular analysis, they may be less significant in explaining the variance captured by Dimension 1.

Dimension 2 (Dim2):

"oldpeak" (ST depression induced by exercise relative to rest): This variable has a strong positive loading on Dimension 2. ST depression is a sign of ischemia and can be an important predictor of coronary artery disease. Therefore, this variable might be relevant in predicting heart disease.

"age": This variable has a slight negative loading on Dimension 2. Age is a well-known risk factor for heart disease, with risk increasing as age increases. The direction and length of the vector for "age" suggest that it does have an influence on Dimension 2, but it might be explained in combination with other variables.

```{r}

fviz_pca(pca)

```


```{r}
# Define colors based on the first variable 
colors <- c(rep("blue", 100), rep("red", 100))  # Adjust as per your actual data

# Plot the first vs. second principal component with dots
plot(x = pca$x[, 1], y = pca$x[, 2], 
     pch = 16,  # Use a solid dot for better visualization
     col = colors,  # Use the defined colors
     xlab = "PC1", ylab = "PC2", main = "First vs. Second PC",
     cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.8)

```


Most of the patients with heart disease (the red ones), are predicted to have a high probability of having heart disease and most of the patients without heart disease (the blue ones) are predicted to have a low probability of having a heart disease.
Thus logistic regression has done a pretty good job. 

#Cluster_Analysis
#K-Means Clustering
```{r}

profiling_num(data2)

```


```{r}

uns_df <- scale(data2[,10:14])
head(as_tibble(uns_df))

```


```{r}

# Calculate distances between observations using the standardized data
distance <- dist(uns_df)

# Visualize the distance matrix
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```




```{r}
k2 <- kmeans(uns_df, 
             center = 2,
             nstart = 25  )

str(k2)
```


```{r}
k2

```


```{r}
fviz_cluster(k2, data = uns_df)

```


```{r}
k3 <- kmeans(uns_df, centers = 3, nstart = 25)
k4 <- kmeans(uns_df, centers = 4, nstart = 25)
k5 <- kmeans(uns_df, centers = 5, nstart = 25)

p1 <- fviz_cluster(k2, geom = "point", data = uns_df)+
  ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = uns_df)+
  ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = uns_df)+
  ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = uns_df)+
  ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1,p2,p3,p2, nrow = 2)

```




#Optimum cluster numbers
```{r}

# Elbow Method
set.seed(123)
fviz_nbclust(uns_df, kmeans, method = "wss")

```


```{r}

# Average Silouette Method
fviz_nbclust(uns_df, kmeans, method = "silhouette")
```


#Gap_ Statistics
```{r}
set.seed(123)

gap_stat <- clusGap(uns_df, FUN = kmeans, nstart = 25, K.max = 10, B = 50)

print(gap_stat, method = "firstmax")

fviz_gap_stat(gap_stat)

```


```{r}
set.seed(123)

final <- kmeans(uns_df, 2, nstart = 25)
final

```

# Descriptive Statistics for Clusters
```{r}
data2[,10:14] %>% 
  mutate(Cluster = final$cluster) %>% 
  group_by(Cluster) %>% 
  summarise_all("mean")

```


# HIERARCHICAL CLUSTERING
```{r}

xquant <- data2[10:14] # Numeric variables
xqual <- data2[2:9]    # Categorical variables
tree <- hclustvar(xquant, xqual)
plot(tree, cex = 0.6)
rect.hclust(tree, k = 4, border = 2:4)

```


```{r}

stab <- stability(tree, B=50) # "B=50" refers to the number of bootstrap samples to use in the estimation.

d <- daisy(data2[-1], metric="gower")

fit <- hclust(d=d, method="complete")    # Also try: method="ward.D"   

plot(fit, cex = 0.6)
rect.hclust(fit, k = 4, border = 2:4)

```



```{r}

kfit <- kmeans(d, 4)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)

```




```{r}

# Perform PCA
pc <- prcomp(uns_df)

# Extract the first two principal components
pc_first_two <- pc$x[, 1:2]

# Perform K-means clustering on the first two principal components
set.seed(12)  # For reproducibility
k <- 2  # Number of clusters
km_clusters <- kmeans(pc_first_two, centers = k)

# Plot the first two principal components with cluster assignments
plot(pc_first_two, col = km_clusters$cluster, 
     main = "First Two Principal Components with Cluster Assignments", 
     xlab = "", ylab = "", pch = 20)


```

```{r}
library(NbClust)
res.nbclust <- data %>% scale() %>% NbClust(distance = "euclidean", min.nc = 2, max.nc = 10, method = "complete", index ="all")
```



#Factor_Analysis

```{r}
# Load necessary library
library(psych)

# Subset the dataset to include only columns 10 to 14
subset_data <- data2[, 10:14]

# Perform Factor Analysis
fit.pc <- principal(subset_data, nfactors = 3, rotate = "varimax")

# View factor analysis results
fit.pc

```




```{r}
round(fit.pc$values, 3)
fit.pc$loadings
```



```{r}
# Communalities
fit.pc$communality

```

```{r}
# Rotated factor scores, Notice the columns ordering: RC1, RC3, RC2 and RC4
fit.pc$scores

```


```{r}
# Play with FA utilities

fa.parallel(data2[, 10:14]) # See factor recommendation

```

```{r}

fa.plot(fit.pc) # See Correlations within Factors

```

```{r}

fa.diagram(fit.pc) # Visualize the relationship

```



```{r}

vss(data2[, 10:14]) # See Factor recommendations for a simple structure

```

```{r}

# Computing Correlation Matrix
corrm.data <- cor(data2[, 10:14])
corrm.data

```

```{r}
plot(corrm.data)
```


```{r}

data2_pca <- prcomp(data2[, 10:14], scale = TRUE)
summary(data2_pca)

```


```{r}

plot(data2_pca)

```


```{r}
# A table containing eigenvalues and %'s accounted, follows. Eigenvalues are the sdev^2
(eigen_data2 <- round(data2_pca$sdev^2,3))

```


```{r}

round(fit.pc$values, 3)

```

```{r}

eigen_data2 <- data2_pca$sdev^2
names(eigen_data2) <- paste("PC", 1:5, sep = "")
eigen_data2
```


```{r}
sumlambdas <- sum(eigen_data2)
sumlambdas

```


```{r}

propvar <- round(eigen_data2/sumlambdas,2)
propvar

```


```{r}
cumvar_data2 <- cumsum(propvar)
cumvar_data2
```

```{r}

matlambdas <- rbind(eigen_data2,propvar,cumvar_data2)
matlambdas

```


```{r}

rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
rownames(matlambdas)

```


```{r}
eigvec.data2 <- data2_pca$rotation
print(data2_pca)
```


```{r}

# Taking the first four PCs to generate linear combinations for all the variables with four factors
pcafactors.data2 <- eigvec.data2[,1:4]
pcafactors.data2

```


```{r}

# Multiplying each column of the eigenvector’s matrix by the square-root of the corresponding eigenvalue in order to get the factor loadings
unrot.fact.data2 <- sweep(pcafactors.data2,MARGIN=2,data2_pca$sdev[1:4],`*`)
unrot.fact.data2

```



```{r}

# Computing communalities
communalities.data2 <- rowSums(unrot.fact.data2^2)
communalities.data2

```


```{r}

# Performing the varimax rotation. The default in the varimax function is norm=TRUE thus, Kaiser normalization is carried out
rot.fact.data2 <- varimax(unrot.fact.data2)
rot.fact.data2

```


```{r}

# The print method of varimax omits loadings less than abs(0.1). In order to display all the loadings, it is necessary to ask explicitly the contents of the object $loadings
fact.load.data2 <- rot.fact.data2$loadings
fact.load.data2

```



```{r}


# Computing the rotated factor scores for the data. Notice that signs are reversed for factors F2, F3, and F4
scale.emp <- scale(data2[, 10:14])
factor_scores <- as.matrix(scale.emp) %*% fact.load.data2 %*% solve(t(fact.load.data2) %*% fact.load.data2)
factor_scores

```



#Multiple_Rgression


```{r}
# Load necessary library
library(psych)

# Subset the dataset to include only columns 10 to 14
subset_data <- data2[, 10:14]

# Perform Factor Analysis
fit.pc <- principal(subset_data, nfactors = 3, rotate = "varimax")

# View factor analysis results
fit.pc

```




```{r}
round(fit.pc$values, 3)
fit.pc$loadings
```



```{r}
# Communalities
fit.pc$communality

```

```{r}
# Rotated factor scores, Notice the columns ordering: RC1, RC3, RC2 and RC4
fit.pc$scores

```


```{r}
# Play with FA utilities

fa.parallel(data2[, 10:14]) # See factor recommendation

```

```{r}

fa.plot(fit.pc) # See Correlations within Factors

```

```{r}

fa.diagram(fit.pc) # Visualize the relationship

```



```{r}

vss(data2[, 10:14]) # See Factor recommendations for a simple structure

```

```{r}

# Computing Correlation Matrix
corrm.data <- cor(data2[, 10:14])
corrm.data

```

```{r}
plot(corrm.data)
```


```{r}

data2_pca <- prcomp(data2[, 10:14], scale = TRUE)
summary(data2_pca)

```


```{r}

plot(data2_pca)

```


```{r}
# A table containing eigenvalues and %'s accounted, follows. Eigenvalues are the sdev^2
(eigen_data2 <- round(data2_pca$sdev^2,3))

```


```{r}

round(fit.pc$values, 3)

```

```{r}

eigen_data2 <- data2_pca$sdev^2
names(eigen_data2) <- paste("PC", 1:5, sep = "")
eigen_data2
```


```{r}
sumlambdas <- sum(eigen_data2)
sumlambdas

```


```{r}

propvar <- round(eigen_data2/sumlambdas,2)
propvar

```


```{r}
cumvar_data2 <- cumsum(propvar)
cumvar_data2
```

```{r}

matlambdas <- rbind(eigen_data2,propvar,cumvar_data2)
matlambdas

```


```{r}

rownames(matlambdas) <- c("Eigenvalues","Prop. variance","Cum. prop. variance")
rownames(matlambdas)

```


```{r}
eigvec.data2 <- data2_pca$rotation
print(data2_pca)
```


```{r}

# Taking the first four PCs to generate linear combinations for all the variables with four factors
pcafactors.data2 <- eigvec.data2[,1:4]
pcafactors.data2

```


```{r}

# Multiplying each column of the eigenvector’s matrix by the square-root of the corresponding eigenvalue in order to get the factor loadings
unrot.fact.data2 <- sweep(pcafactors.data2,MARGIN=2,data2_pca$sdev[1:4],`*`)
unrot.fact.data2

```



```{r}

# Computing communalities
communalities.data2 <- rowSums(unrot.fact.data2^2)
communalities.data2

```


```{r}

# Performing the varimax rotation. The default in the varimax function is norm=TRUE thus, Kaiser normalization is carried out
rot.fact.data2 <- varimax(unrot.fact.data2)
rot.fact.data2

```


```{r}

# The print method of varimax omits loadings less than abs(0.1). In order to display all the loadings, it is necessary to ask explicitly the contents of the object $loadings
fact.load.data2 <- rot.fact.data2$loadings
fact.load.data2

```



```{r}


# Computing the rotated factor scores for the data. Notice that signs are reversed for factors F2, F3, and F4
scale.emp <- scale(data2[, 10:14])
factor_scores <- as.matrix(scale.emp) %*% fact.load.data2 %*% solve(t(fact.load.data2) %*% fact.load.data2)
factor_scores



```


#Logistic_Rgression

# Now we see how many rows of data have NA values
```{r}
nrow(data2[is.na(data2$ca) | is.na(data2$thal), ])

```

There is no rows of data having NAs in them.



It is now necessary to ensure that samples of both healthy and diseased individuals are taken from each gender (male and female). We should presumably eliminate every female from the model if heart disease is limited to male samples.
```{r}

xtabs(~ target + sex, data=data2)

```
Healthy and Unhealthy patients are both represented by a lot of female and male samples.



For all 3 levels of chest pain reported by patients
```{r}

xtabs(~ target + cp, data=data2)

```



```{r}

xtabs(~ target + fbs, data=data2)


```


```{r}
xtabs(~ target + restecg, data=data2)

```


```{r}
xtabs(~ target + exang, data=data2)
```


```{r}
xtabs(~ target + slope, data=data2)
```


```{r}
xtabs(~ target + ca, data=data2)
```


```{r}
xtabs(~ target + thal, data=data2)
```



#LDA(Linear_Discrimination_Analysis)

```{r}
Ldata <- as.matrix(data2[, c(10:14)])
row.names(Ldata) <- row.names(data2)
Lraw <- cbind(Ldata, as.numeric(as.factor(data2$target)) - 1)
colnames(Lraw)[ncol(Lraw)] <- "diagnosis"
smp_size_raw <- floor(0.75 * nrow(Lraw))
train_ind_raw <- sample(nrow(Lraw), size = smp_size_raw)
train_raw.df <- as.data.frame(Lraw[train_ind_raw, ])
test_raw.df <- as.data.frame(Lraw[-train_ind_raw, ])
Lraw.lda <- lda(formula = diagnosis ~ ., data = train_raw.df)
Lraw.lda
```


```{r}
summary(Lraw.lda)
```




```{r}
print(Lraw.lda)
```


```{r}
plot(Lraw.lda)
```


```{r}

# Make predictions on the test data
Lraw.lda.predict <- predict(Lraw.lda, newdata = test_raw.df)

# Extract predicted classes
Lraw.lda.predict$class
```

```{r}
Lraw.lda.predict$x
```

```{r}
# Get the predicted classes
predicted_classes <- predict(Lraw.lda, newdata = test_raw.df)$class

# Calculate accuracy
accuracy <- mean(predicted_classes == test_raw.df$diagnosis)
accuracy
```


```{r}
# Get the posteriors as a dataframe
Lraw.lda.predict.posteriors <- as.data.frame(predict(Lraw.lda, newdata = test_raw.df)$posterior)

# Calculate the performance metrics
pred <- prediction(Lraw.lda.predict.posteriors[, 2], test_raw.df$diagnosis)
roc.perf <- performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")@y.values

# Plot the ROC curve
plot(roc.perf)
abline(a = 0, b = 1)
text(x = 0.25, y = 0.65, paste("AUC = ", round(auc.train[[1]], 3), sep = ""))
```



#Logistic_Regression
```{r}
library(tidyverse)
library(readr)
library(ROCR)
library(PerformanceAnalytics)
library(e1071)
library(caret)
library(gbm)
library(corrplot)
library(ggcorrplot)
library(MASS)
library(rpart)
library(caTools)
library(naivebayes)
library(class)
library(ISLR)
library(glmnet)
library(Hmisc)
library(funModeling)
library(pROC)
library(randomForest)
library(klaR)
library(scales)
library(cluster)
library(factoextra)
library(DataExplorer)
library(ClustOfVar)
library(GGally)
library(dplyr)
library(visdat)
```


```{r}

file_path <- "D:/Multivariate Analysis/Final_Project/heart.csv"
data <- read.csv(file_path)
head(data)
```



#Transformation
```{r}

data2 <- data %>% 
  mutate(sex = if_else(sex == 1, "MALE", "FEMALE"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "YES" ,"NO"),
         cp = if_else(cp == 1, "ATYPICAL ANGINA",
                      if_else(cp == 2, "NON-ANGINAL PAIN", "ASYMPTOMATIC")),
         restecg = if_else(restecg == 0, "NORMAL",
                           if_else(restecg == 1, "ABNORMALITY", "PROBABLE OR DEFINITE")),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal),
         target = if_else(target == 1, "UNHEALTHY", "HEALTHY")
         ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())

```



```{r}

str(data2)


```



# Now we see how many rows of data have NA values
```{r}
nrow(data2[is.na(data2$ca) | is.na(data2$thal), ])

```

There is no rows of data having NAs in them.



It is now necessary to ensure that samples of both healthy and diseased individuals are taken from each gender (male and female). We should presumably eliminate every female from the model if heart disease is limited to male samples.
```{r}

xtabs(~ target + sex, data=data2)

```
Healthy and Unhealthy patients are both represented by a lot of female and male samples.



For all 3 levels of chest pain reported by patients
```{r}

xtabs(~ target + cp, data=data2)

```



```{r}

xtabs(~ target + fbs, data=data2)


```


```{r}
xtabs(~ target + restecg, data=data2)

```


```{r}
xtabs(~ target + exang, data=data2)
```


```{r}
xtabs(~ target + slope, data=data2)
```


```{r}
xtabs(~ target + ca, data=data2)
```


```{r}
xtabs(~ target + thal, data=data2)
```


#Logistic Rgression
A simple model in which we will try to predict heart disease only using the gender of eah patient.
```{r}

logistic <- glm(target ~ sex, data = data2, family = "binomial")
summary(logistic)
```
Here the glm() function that performs Generalized Linear Models.
we used formula syntax(hd~sex) to predict heart disease(target).
Then we specify the data(data=data2) that we are using for this model.
In last, we specify that we want the binomial family of generalized linear models. This makes the glm() function do logistic regression, as apposed to some other type of generalized linear model.
we are storing the output from glm() in a variable called "logistic".
Then use the summary function to get details about the logistic regression.

summary output:
The first line has the original call to the glm() function.
Then it gives the summary of the deviance residuals.They look good since they are close to being centered on 0 and are roughly symmetrical.

Then we have the coefficients. They corresponding to the following model: 
Heart disease = 0.9662 -1.2859 x the patient is male.
The variable(the patient is male, is equal to 2 when the patient is female and 1 when the patient is male ).
Thus if are predicting heart disease for a female patient, we get the following equation:
Heart disease = 0.9662 -1.2859 x 2 = -1.6056.
Thus the log (odds) that a female has heart disease = -1.6056
If are predicting heart disease for a male patient, we get the following equation:
Heart disease = 0.9662 -1.2859 x 1 = -0.3197.
Since the first term(0.9662) is the log(odds) of a female having heart disease and the second term(-1.2859) indicates the increase in the log(odds) that a male has of having heart disease. In other words, the second term(-1.2859) is the log(odds ratio) of the odds that a male will have heart disease over the odds that a female will have heart disease.

The output std.error and z value shows how the Walds test was computed for both coefficients.

The p-values(Pr(>|z|)) are well below 0.05 and thus, the log(odds) and the log(odd ratio) are both statstically significant.

When we do "normal linear regression , we estimate both the mean and variance  from the data". In contrast, with logistic regression, we estimate the mean of the data, and the variance is derived from the mean. Since we are not estimating the variance from the data(instead of just deriving it from the mean), it is possible that the variance is underestimated.
If so, you can adjust the dispersion parameter in the summary command.

The null deviance and residuals deviance can be used to compare models , compute R^2 and an overall P-value.

Lastly, we have the number of Fisher Scoring iterations, which just tell us how quickly the glm()  function converged on the maximum likelihood estimates for the coefficients


```{r}
median(data2$age)
```

```{r}

logistic <- glm(target~ sex, data=data2, family= "binomial")

```
we have done a simple logistic regression using just one of the variables (sex) to predict heart disease, we can create a fancy model that uses all of the variables to predict heart disease.

```{r}

logistic <- glm(target~ ., data=data2, family= "binomial")
summary(logistic)
```
This formula syntax, target~., means that we want to model heart disease(target) using all of the remaining variables in our data.frame called "data2".

From the above result, we will just talk about a few of the coefficients:
We see that age is not a useful predictor because it has a large P-value. However the median age in our dataset was 56, so most of the folks were pretty old and that explains why its was not very useful.

The predictors with p-values less than 0.05 are: sexMALE, exangYES, cpNON-ANGINAL PAIN, ca1, ca2, ca3, thal1, thal2, age,
trestbps, chol, thalach, oldpeak.

On the bottom of the output, we see that the Residual Deviance and the AIC are much smaller for this fancy model than they were for the simple model, when we only used gender to predict heart disease.

If we want to calculate the  McFadden's Pseudo R^2 , we can pull the log -likelihood of the null model out of the logistic variable by getting the value for the null deviance and dividing by -2 and we can pull the log-likelihood for the fancy model out of the logistic variable by getting the value for the residual deviance and dividing by -2.
```{r}

ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
 
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null

```

Then we just do the math and ended up with a Pseudo R^2 = 0.54. This can ne interpreted as the overall effect size. And we can use the same log_likelihoods to calculate a P_valuefor that R^2 using a Chi-square distribution.

```{r}

1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))

```

In this case, the tiny P-value indicates that the R^2 value is not a result of random chance.



Lastly, we can draw a graph that shows the predicted probabilities that each patient has heart disease along with their actual heart disease status. 
```{r}
## Load necessary libraries
library(ggplot2)
library(cowplot)

## Create a data frame with predicted probabilities and actual heart disease status
predicted.data <- data.frame(
  probability.of.target = logistic$fitted.values,
  target = ifelse(data$target == 0, "Healthy", "Unhealthy")
)

## Order the predicted data by probability of target, and add a rank column (sort the data.frame from low probabilities to high probabilities)
predicted.data <- predicted.data[order(predicted.data$probability.of.target, decreasing = FALSE), ]
# Then we add a new column to the data.frame that is rank of each sample , from low probabability to high probability.
predicted.data$rank <- 1:nrow(predicted.data)

## Plot the predicted probabilities for each sample, color by actual heart disease status
ggplot(data = predicted.data, aes(x = rank, y = probability.of.target)) +
  geom_point(aes(color = target), alpha = 1, shape = 4, stroke = 2) +
  scale_color_manual(values = c("Healthy" = "blue", "Unhealthy" = "red")) +  ## Customize colors
  xlab("Index") +
  ylab("Predicted probability of heart disease") +
  theme_minimal()

## Save the plot as a PDF
ggsave("heart_disease_probabilities.pdf")



```

Most of the patients with heart disease (the red ones), are predicted to have a high probability of having heart disease and most of the patients without heart disease (the blue ones) are predicted to have a low probability of having a heart disease.
Thus logistic regression has done a pretty good job. 



